{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Computation Coursework- Group 16\n",
    "\n",
    "Jon Duffy, Ed Wong, Shiyu Fan, Yawen Sun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "For this task we look into using a neural network to classify and localise objects in images. We have X number of preprocessed training images of dimension 400x400 pixels, each of which are paired with a text file that specify classes and, if present, bounding boxes around a present object in that image. These images are from the VOC data set.\n",
    "\n",
    "We also have Y number of testing images of the same format. Our task is to accurately generate an accompanying text file, which specifies the classes and, if present, bounding boxes of all the test images.\n",
    "\n",
    "Below is the text file for image 2007_000027. Here we can see 20 classes.\n",
    "The “1 0” following the class name indicates that this class is not present in the image.\n",
    "If an object has been detected, the numbers that follow indicate the bounding box with pixel references. The pixels are numbered from top to bottom, then left to right. For example, 1 is pixel (1,1), 2 is pixel (2,1), etc. The numbers are paired such that the first number represents the starting pixel and the second represents how many consecutive pixels after this. This creates a bounding box around our objects."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2007_000027.jpg_aeroplane,1 0\n",
    "2007_000027.jpg_bicycle,1 0\n",
    "2007_000027.jpg_bird,1 0\n",
    "2007_000027.jpg_boat,1 0\n",
    "2007_000027.jpg_bottle,1 0\n",
    "2007_000027.jpg_bus,1 0\n",
    "2007_000027.jpg_car,1 0\n",
    "2007_000027.jpg_cat,1 0\n",
    "2007_000027.jpg_chair,1 0\n",
    "2007_000027.jpg_cow,1 0\n",
    "2007_000027.jpg_diningtable,1 0\n",
    "2007_000027.jpg_dog,1 0\n",
    "2007_000027.jpg_horse,1 0\n",
    "2007_000027.jpg_motorbike,1 0\n",
    "2007_000027.jpg_person,32143 144 32543 144 32943 144 33343 144 33743 144 34143 144 34543 144 34943 144 35343 144 35743 144 36143 144 36543 144 36943 144 37343 144 37743 144 38143 144 38543 144 38943 144 39343 144 39743 144 40143 144 40543 144 40943 144 41343 144 41743 144 42143 144 42543 144 42943 144 43343 144 43743 144 44143 144 44543 144 44943 144 45343 144 45743 144 46143 144 46543 144 46943 144 47343 144 47743 144 48143 144 48543 144 48943 144 49343 144 49743 144 50143 144 50543 144 50943 144 51343 144 51743 144 52143 144 52543 144 52943 144 53343 144 53743 144 54143 144 54543 144 54943 144 55343 144 55743 144 56143 144 56543 144 56943 144 57343 144 57743 144 58143 144 58543 144 58943 144 59343 144 59743 144 60143 144 60543 144 60943 144 61343 144 61743 144 62143 144 62543 144 62943 144 63343 144 63743 144 64143 144 64543 144 64943 144 65343 144 65743 144 66143 144 66543 144 66943 144 67343 144 67743 144 68143 144 68543 144 68943 144 69343 144 69743 144 70143 144 70543 144 70943 144 71343 144 71743 144 72143 144 72543 144 72943 144 73343 144 73743 144 74143 144 74543 144 74943 144 75343 144 75743 144 76143 144 76543 144 76943 144 77343 144 77743 144 78143 144 78543 144 78943 144 79343 144 79743 144 80143 144 80543 144 80943 144 81343 144 81743 144 82143 144 82543 144 82943 144 83343 144 83743 144 84143 144 84543 144 84943 144 85343 144 85743 144 86143 144 86543 144 86943 144 87343 144 87743 144 88143 144 88543 144 88943 144 89343 144 89743 144 90143 144 90543 144 90943 144 91343 144 91743 144 92143 144 92543 144 92943 144 93343 144 93743 144 94143 144 94543 144 94943 144 95343 144 95743 144 96143 144 96543 144 96943 144 97343 144 97743 144 98143 144 98543 144 98943 144 99343 144 99743 144 100143 144 100543 144 100943 144 101343 144 101743 144 102143 144 102543 144 102943 144 103343 144 103743 144 104143 144 104543 144 104943 144 105343 144 105743 144 106143 144 106543 144 106943 144 107343 144 107743 144 108143 144 108543 144 108943 144 109343 144 109743 144 110143 144 110543 144 110943 144 111343 144 111743 144\n",
    "2007_000027.jpg_pottedplant,1 0\n",
    "2007_000027.jpg_sheep,1 0\n",
    "2007_000027.jpg_sofa,1 0\n",
    "2007_000027.jpg_train,1 0\n",
    "2007_000027.jpg_tvmonitor,1 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This text file corresponds to this image:\n",
    "\n",
    "<img src=\"files/2007_000027.jpg\">\n",
    "\n",
    "Classification and localisation is an active area of research within machine learning. The VOC data set that we have here is the standard benchmark to measure the performance of classification of neural networks. \n",
    "\n",
    "We aim to experiment with (WHAT ARE WE DOING) to improve the classification and localization of the neural network as well as understanding the impact of these changes on this classification and localization problem. The quality of the neural network will be measured by various cross validation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design\n",
    "Our initial approach was to create a neural network from scratch so we could experiment with all of the details of the network, such as the topology, applying different types of layers (such as fully connected layers, convolutional layers), learning rates etc. However, there were a few disadvantages/bottlenecks that we came across:\n",
    "Approaching this problem by working from the ground up leaves much more room for error.\n",
    "Implementing a network from scratch means that we must train all of our weights and biases using many epochs of our training set. With object detection and classification, it is generally accepted that deep neural networks are better at this task than shallow neural networks, resulting in many more parameters to calculate and iterate. As we have 14,626 training images, this presents a computational resource bottleneck. Even if we were to utilise cloud based services such a google cloud, or amazon web services, we will still run into another barrier.\n",
    "Namely, time. This was a precious resource in this project. According to CITE, a good neural network could take months to train to a good standard from scratch, even with many GPUs.\n",
    "As a result we decided to change our approach to take an existing network with pretrained weights, and update these weights by training with our dataset.\n",
    "Researching Existing Networks\n",
    "When researching existing Neural Networks, we came across many different variations. For instance, R-CNN and SSD300 to name a couple. However, a neural network that caught our eye was [YOLOv2](https://pjreddie.com/darknet/yolo/), because of [this video](https://www.youtube.com/watch?v=VOC3huqHrss) demonstration that applied this neural network to a movie clip in real time. This video clip showcased the speed of this network- a factor which was crucial.\n",
    "\n",
    "After some further research into this neural network, we analysed the topology of it. It consisted of 24 convolutional layers followed by 2 fully connected layers. We could experiment with the topology of the network but adding layers, removing layers, changing the dimensions of the layers etc. However, due to time constraints, we instead focus on fixing this topology and altering other features, ie the hyperparameters of the network.\n",
    "\n",
    "Pretrained weights from ImageNet were used on the convolutional layers. This essentially means that they only need to train the weights and biases in the final two fully connected layers, which saves time and resources. We could also implement a similar approach by training an implementation of this network on our training data with pretrained weights on the convolutional layers and we find the weights and biases on the fully connected layer. However due to time limitations, it is more logical to take the YOLOv2 network and train it in the network’s current state to update the current weights by training it on few epochs with our own training set).\n",
    "\n",
    "Let us briefly touch upon how this network is used in order to both classify and localise objects in an image. First, the image is divided into an equal sized grid, e.g. 9x9. Inside each grid cell, the network assigns several anchor boxes. These anchor boxes then attempt to locate an object. The reason for multiple anchor boxes per cell is so we can classify multiple objects in a single cell (Clearly, anchor boxes limit the number of objects in the same grid cell). After this, it implements a certain loss function that balances the trade off between location error and classification error. This loss value is then used to implement back propagation.\n",
    "\n",
    "The output of the network is a high dimensional tensor with each component of the tensor referring to the grid cell, the boxes, the class and the probability of that class occuring.\n",
    "\n",
    "The layers of the network consist of convolutional layers (which extract the features) and end with fully connected layers (which calculate the probabilities). This information was found by exploring the network, and from [this paper](https://arxiv.org/abs/1506.02640).\n",
    "\n",
    "We found a python implementation of YOLOv2 called [YAD2K](https://github.com/allanzelener/YAD2K). This uses the YOLOv2 network design but uses Keras, tensorflow and numpy.\n",
    "How we will Train:\n",
    "The training cycle worked through 3 different stages. Each stage split the data into 1800 training images and 200 validation images:\n",
    "The first stage was 5 epochs. [First it finetunes only the last layer]\n",
    "The second 30 epochs. [tunes all layers since it need not save weights at end of each epoch it runs without early stopping and checkpointing\n",
    "The third 30 epochs. [runs with checkpoint and early stopping to stop as soon as val loss starts increasing.]\n",
    "We seperate whole set of images into 8 subsets. Each of these subset contains 2000 images.\n",
    "Our Version Of This Network\n",
    "We aim to change the following features and analyse how they effect the networks ability to detect and classify:\n",
    "Activation functions\n",
    "Anchor box coordinates\n",
    "Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation- input code\n",
    "Describe how you implemented your neural network and the associated performance analysis mechanisms.  Explain  why  you  chose  to  do  it  that  way.   Remember  to  cite  any sources you used.  Include comments in the source code that explains how the code works.  [20%]\n",
    "Parser for File Types and Data Format\n",
    "In order to implement the neural network we have chosen, our first task was to write a parser so we can understand the annotation files. Then, we needed to write some code to modify the output so the network generates the specific format we require.\n",
    "\n",
    "<convert.py>\n",
    "\n",
    "<test_copy.py>\n",
    "\n",
    "However, the data that we have actually has some room for interpretation in some cases. For example, if there are many overlapping objects of the same class, the data we have means that we cannot explicitly decipher between two objects in certain circumstances. We can visualise our data as below. We state three examples where this becomes an issue:\n",
    "<img src=\"files/figures/figure3_1.png\">\n",
    "Top left: Original\n",
    "Top right: We do not know if there is an object inside the “silhouette”\n",
    "Bottom: We do not know how fair this box extends to.\n",
    "\n",
    "It would be rare that this situation will indeed occur, however it should still be noted when converting the data format.\n",
    "\n",
    "The data format that we are converting to and from is much more efficient and robust than the data format we have. It simply states a set of coordinates and dimensions per object in an image. For example, we can express the example we had in the introduction as:\n",
    "2007_000027.jpg_person, 143 80 287 279\n",
    "Where each integer refers to (x_1, y_1, x_2, y_2) which is the bounding box around an object of that class in that image.\n",
    "\n",
    "We can pinpoint where we lose information in our code for test_yolo.py\n",
    "\n",
    "A further issue we found when testing was that the network would not perform correctly when given a greyscale image. Therefore, we are required to resave each black and white image so our network can take it as input.\n",
    "Computational Resources\n",
    "Training a neural network to classify and localise images requires a large amount of computational resource. We quickly learnt that it was not feasible to train on our laptops for two reasons. Firstly, the previously mentioned amount or resource required meant that it would take an infeasible time given the time constraints of this assignment, and secondly running from a laptop requires the laptop to be open and awake for processes to run. We decided to turn to the cloud to solve this problem. This allowed us to rent a powerful virtual machines with professional GPU’s (rather than gaming GPU’s found in consumer desktop/ laptops). We also installed and configured Jenkins on our VM. https://jenkins.io/ Jenkins is a job orchestration tool that can be controlled via a web interface. This allowed us to define and schedule jobs on the neural network. As the jenkins user is running the tasks, were we able to not worry about maintaining an open session and could check the status of Job’s via the web UI. \n",
    "    We initially installed on Amazon Web Services (AWS) however there were $300 worth or free credits available for setting up a new account on Google Cloud (GC). AWS provides Amazon machine images (AMI’s) that are pre built and ready for machine learning, with many key tools, such as tensorflow and keras pre installed. This made setup on AWS very fast. However when we moved to GC we had to install display drivers and many tools before we could get started. \n",
    "Even when training on a powerful cloud instance with 80GB or RAM, we trying to train the network on all the images, RAM would max out within seconds and the process would get killed by the operating system. To work around this we broke the training data into smaller batches and trained on these. Jenkins made this easy as we were able to queue up many training jobs, and we were able to check the web UI anytime of day to get updated on progress.\n",
    "During training we came across a bug in the the YAD2K code. When drawing the bounding boxes on images during the training cycle, it encountered a type mismatch bug, int -> float. Updating a single function argument fixed this issue and training jobs were able to then able to run successfully. \n",
    "\n",
    "We retrain 2000 images 8 times and combine them into yolo.weights for testing.\n",
    "Performance Analysis:\n",
    "We will discuss an accuracy measurements which is used to give confidence for a particular object. First, we introduce the notion of “Intersection over Union” (or IoU). While training our network, we can judge our results and analyse the accuracy because we have ground truth boxes. Below, we see that the green box is the position of object what we can get from training data (ground truth) and the red one is our predicted position after training.\n",
    "<img src=\"files/figures/figure3_2.png\">\n",
    "This visual representation from Adrian Bosebrock (2016) demonstrates the usefulness of IOU measurements.\n",
    "<img src=\"files/figures/figure3_3.png\">\n",
    "With this IOU value, we multiply it by the probability that the object is of that certain class:\n",
    "Pr(Class_i) ∗ IOU^truth_pred\n",
    "<img src=\"files/figures/figure3_4.png\">\n",
    "From this figure, our network is 80% confident that the object in pink box is a truck. Our aim is to maximise this confidence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "Describe the experiments you carried out to optimise your network’s generalization performance, and present the results you obtained.  Explain in detail how you used the training and testing data sets.  The results should be presented in a statistically rigorous manner.  [45%]\n",
    "Now that we have a functioning network that we can train, we turn to experimenting. Let’s recap what we will be experimenting with:\n",
    "Varying Activation Functions\n",
    "Adjusting Anchor Box Sizes\n",
    "Learning Rates\n",
    "Varying Activation Functions:\n",
    "Our exploration of the network began with the activation functions. While looking through the code of this neural network, we changed the activation functions of the network and ran some tests to see how it would affect the output. However, this clearly makes little logical sense as changing the activation layer will be altering the network which has been trained on a certain data set. Altering functions in a pre-trained network should have bad implications on the accuracy of the outputs.\n",
    "\n",
    "The original activation functions were “leaky” (with gradient 0.1), with the exception of the penultimate layer where the activation function was “linear”. We changed these activation functions (and their derivatives) and tried implementing the following:\n",
    "Changed all activations to “linear”.\n",
    "Changed to Relu activations.\n",
    "Changed to leaky activation with varying gradients (0.0001, 0.05, 0.11, 0.105, 0.1075, 0.115, 0.2, 0.5, 0.8)\n",
    "These changes were completely at random and their purpose was simply to see what the network would do with these changes.\n",
    "\n",
    "When we made these changes, we found that some interesting results. Unsurprisingly, many of these functions completely over classified in the wrong areas, resulting in completely ludicrous outputs. On the other hand, some of the outputs classified nothing at all. These outputs are to be expected because of the points stated above.\n",
    "\n",
    "However, something unexpected happened when we used a leaky activation with gradient 0.11- only a slight variant on the original. On the particular image we were testing, the network managed to detect a motorbike in the top left hand side- an object that was not detected by the original trained network!\n",
    "\n",
    "Although the bounding boxes on the originally detected objects were slightly a skew, it was a surprise to see that this network seemed better on detection. What seemed to happen is that the there was a decrease of the threshold of detection, allowing for more questionable objects to be detected that were in fact correct. However this was only tested on a single image and should be explored further before drawing any conclusions.\n",
    "\n",
    "Below are the results.\n",
    "\n",
    "Normal yolo.cfg:\n",
    "<img src=\"files/figures/figure4_1.png\">\n",
    "Linear activations:\n",
    "<img src=\"files/figures/figure4_2.png\">\n",
    "Relu activations and leaky activation with 0.0001 and 0.05 (just didn’t classify anything):\n",
    "<img src=\"files/figures/figure4_3.png\">\n",
    "Leaky with 0.5:\n",
    "<img src=\"files/figures/figure4_4.png\">\n",
    "Leaky with 0.8:\n",
    "<img src=\"files/figures/figure4_5.png\">\n",
    "Leaky with 0.2:\n",
    "<img src=\"files/figures/figure4_6.png\">\n",
    "Leaky 0.11:\n",
    "<img src=\"files/figures/figure4_7.png\">\n",
    "Leaky 0.115:\n",
    "<img src=\"files/figures/figure4_8.png\">\n",
    "Leaky 0.105:\n",
    "<img src=\"files/figures/figure4_9.png\">\n",
    "Leaky 0.1075:\n",
    "<img src=\"files/figures/figure4_10.png\">\n",
    "We can analogise changing the network after it has been trained to moving football goal posts after a ball has been kicked. With this, it is incredibly difficult or impossible to mathematically reason that your network is going in the right direction (or our goal posts will aline with the trajectory of the ball).\n",
    "\n",
    "Unfortunately, due to time constraints, we were unable to explore this branch further and delve into the mathematics of why this happens. Regardless, it was an interesting aside to “move the network to optimize” instead of the traditional gradient descent approach.\n",
    "Varying Anchor Box Sizes:\n",
    "A possible parameter we could experiment with is the number of anchor boxes and their dimensions. We tackle this by considering several approaches:\n",
    "The first could be just to pick varying sizes by hand. This method will come with it’s obvious disadvantages as it is not statistically rigorous. We omit this method.\n",
    "An alternative is to use k-means algorithm to select the number of boxes. This is a possible option, however due to lack of time, implementing this algorithm may hinder progress so we choose to not use this. YOLOv2 uses 5 anchor boxes, and according to [Vivek Yadav](https://medium.com/@vivek.yadav/part-1-generating-anchor-boxes-for-yolo-like-network-for-vehicle-detection-using-kitti-dataset-b2fe033e5807), after some k-means experimentation, the conclusion is that 5 anchor boxes is the best for this network (note they are using a different dataset but the general idea is the same).\n",
    "Use k-means algorithm to select the dimensions of the boxes. As we are choosing 5 anchor boxes, we could run 5-means algorithm to calculate the best dimensions for these anchor boxes. However, our data set is very similar to the data set that this network has already been trained on, so it would make sense to omit this experimentation as it has already been optimized to this factor and save some time.\n",
    "The 5 anchor box dimensions we will use are:\n",
    "(    (0.57273, 0.677385),\n",
    "(1.87446, 2.06253),\n",
    "(3.33843, 5.47434),\n",
    "(7.88282, 3.52778),\n",
    "(9.77052, 9.16828)    )\n",
    "Learning Rate:\n",
    "Different learning rates to try:\n",
    "Control- using the original parameters (0.001)\n",
    "0.0001\n",
    "0.0005\n",
    "0.0015\n",
    "0.0025\n",
    "A note on the Number of Epochs:\n",
    "Initially, we thought that we could experiment with the number of epochs that our network runs through. However, our network implementation already uses “early stopping” which ensure that a network does not overfit to a certain dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "Summarize your key findings, including which factors  proved  most  crucial,  and  what  was  the  best  generalization performance you achieved.  [10%]\n",
    "From our experiments and implementations, we have found that \n",
    "FINDINGS FROM EXPERIMENTS\n",
    "\n",
    "Our findings could be improved if we had more time and resources,\n",
    "More computational power to have the ability to train these networks to our own training data, from scratch.\n",
    "\n",
    "We could also designed our own datasets consisting of simple data, such as coloured shapes, to see how well our network generalises (is this what I mean?).\n",
    "\n",
    "Also, it is important to note tha\n",
    "\n",
    "## Description of contribution\n",
    "Describe each group member's’ contribution to the overall project. [5%]\n",
    "Jon- \n",
    "Ed-\n",
    "Shiyu- \n",
    "Yawen- \n",
    "That dude we never met- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
